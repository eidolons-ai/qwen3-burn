# CLAUDE.md

## Project Overview

qwen3-burn is a Rust library implementing Qwen3 LLM inference using the Burn 0.20 deep learning framework. It loads HuggingFace SafeTensors weights directly and runs on multiple backends (WGPU/Metal, NdArray/CPU, CUDA).

## Architecture

Qwen3 is a decoder-only transformer with these distinguishing features vs Llama:
- **QK-Norm**: RMSNorm applied to Q and K projections *before* RoPE (unique to Qwen3)
- **GQA**: Grouped-query attention with consistently 8 KV heads (dense) or 4 KV heads (MoE) across model sizes
- **SwiGLU**: `down_proj(silu(gate_proj(x)) * up_proj(x))`
- **RoPE**: theta=1,000,000, head_dim=128
- **RMSNorm**: eps=1e-6 (Llama uses 1e-5)
- **No bias** in any linear layers
- **vocab_size**: 151,936 (dense) or 152,064 (235B MoE)
- **tie_word_embeddings**: true for <=4B and 30B-A3B, false for >=8B and 235B-A22B
- **MoE** (30B-A3B, 235B-A22B): Router + 128 expert FFNs per layer, top-8 routing per token

## Module Layout

```
src/
  lib.rs           # Re-exports: model, sampling, tokenizer (cache & transformer are pub(crate)); bench_internals feature gate
  model.rs         # Qwen3Config, Qwen3<B> (generate/generate_streaming), StopReason, GenerationEvent, GenerationParams, SafeTensors loading
  transformer.rs   # Transformer, TransformerBlock, Mlp (Dense/Moe), MoeLayer, MultiHeadAttention, FeedForward, RmsNorm, RotaryEmbedding
  cache.rs         # KvCache - pre-allocated [batch, heads, max_seq, head_dim] with sliding window
  sampling.rs      # Sampler enum: TopP (nucleus) and Argmax
  tokenizer.rs     # Qwen3Tokenizer wrapping HF `tokenizers` crate
examples/
  chat.rs          # CLI chat app with clap, feature-gated backend selection
benches/
  ops.rs           # Criterion benchmarks for all core ops (NdArray backend)
```

## Weight Loading

PyTorch stores Linear weights as `[out_features, in_features]`; Burn stores as `[in_features, out_features]`. All Linear weights are **transposed** during loading (`take_linear_weight`). Embedding weights are loaded without transpose (`take_tensor_2d`). When `tie_word_embeddings=true`, the embedding weight is transposed and used as the lm_head weight.

Weight loading streams shard-by-shard: after reading each safetensors file, completed layers are loaded into the model immediately and their f32 data is freed via `remove()` from the tensor map. This keeps peak memory close to the final model size (~1x) rather than ~2-3x.

SafeTensors key mapping (dense layers):
- `model.embed_tokens.weight` -> Embedding
- `model.layers.{i}.self_attn.{q,k,v,o}_proj.weight` -> Linear (transposed)
- `model.layers.{i}.self_attn.{q,k}_norm.weight` -> RmsNorm (1D)
- `model.layers.{i}.mlp.{gate,up,down}_proj.weight` -> Linear (transposed)
- `model.layers.{i}.{input,post_attention}_layernorm.weight` -> RmsNorm (1D)
- `model.norm.weight` -> final RmsNorm
- `lm_head.weight` -> Linear (transposed, or tied with embedding)

SafeTensors key mapping (MoE layers):
- `model.layers.{i}.mlp.experts.{j}.gate_proj.weight` -> 2D `[moe_intermediate, hidden]` (per-expert, transposed and packed with up_proj into 3D gate_up_proj)
- `model.layers.{i}.mlp.experts.{j}.up_proj.weight` -> 2D `[moe_intermediate, hidden]` (per-expert, transposed and packed with gate_proj)
- `model.layers.{i}.mlp.experts.{j}.down_proj.weight` -> 2D `[hidden, moe_intermediate]` (per-expert, transposed and stacked into 3D)
- `model.layers.{i}.mlp.gate.weight` -> 2D `[num_experts, hidden]` (router, no transpose)

## Build & Test

```bash
# Check (fast)
cargo check --features wgpu

# Build release
cargo build --release --features wgpu --example chat

# Run tests (no GPU or model weights needed — uses ndarray backend via dev-dependencies)
cargo test

# Lint
cargo fmt -- --check
cargo clippy --all-targets

# Benchmarks (CPU, no model weights needed — uses ndarray backend + "bench" feature)
cargo bench --features bench              # All benchmarks
cargo bench --features bench -- rms_norm  # Single group
cargo bench --features bench -- "attention/decode"  # Subset

# Run (needs model files in a directory: config.json, tokenizer.json, *.safetensors)
cargo run --release --features wgpu --example chat -- \
  --model-path ./models/Qwen3-0.6B --prompt "Hello" --max-tokens 100

# Download model files (Python)
python3 -c "from huggingface_hub import snapshot_download; snapshot_download('Qwen/Qwen3-0.6B', local_dir='./models/Qwen3-0.6B', allow_patterns=['*.safetensors', 'config.json', 'tokenizer.json'])"
```

Backend features are mutually exclusive: `wgpu` (Metal on macOS), `ndarray` (CPU), `cuda`. The `bench` feature is independent and only enables `bench_internals` re-exports for the benchmark harness.

## Commit Style

Do not include model or agent attribution in commit messages (no "Co-Authored-By", "Generated by", etc.).

Tests use `NdArray` backend (CPU) via dev-dependencies and require no model weights. 52 unit tests cover config parsing, streaming types, RmsNorm, RoPE, causal mask, KV cache (including overflow/truncation edge cases), MoE routing, sampling, and end-to-end shape verification for both dense and MoE transformer blocks.

Criterion benchmarks (`benches/ops.rs`) measure all core ops parameterized by sequence length using Qwen3-0.6B dimensions. 48 benchmarks across 8 groups: rms_norm, rope, feed_forward, moe_layer, attention (prefill + decode), transformer_block (prefill + decode), causal_mask, kv_cache. Stateful ops (attention, transformer_block, kv_cache) use `iter_batched` with fresh caches per iteration. HTML reports are written to `target/criterion/`.

## Key Implementation Details

- RmsNorm operates on 3D tensors `[batch, seq, hidden]`, computing variance over dim 2
- QK-Norm reshapes Q/K to `[batch*seq*heads, 1, head_dim]` to apply RmsNorm on head_dim, then reshapes back
- RoPE precomputes cos/sin tables at init; slices per-position during forward
- Causal mask is built on CPU as a float tensor with 0.0 / -inf values, shaped `[q_seq, kv_seq]`, then unsqueezed to `[1, 1, q_seq, kv_seq]` for broadcasting
- KV cache uses pre-allocated tensors with `slice_assign` for appending; sliding window shifts when exceeding max_seq_len; chunks larger than max_seq_len are truncated to the last max_seq_len tokens
- Generation: `generate_streaming()` is the core method; `generate()` is a convenience wrapper. Both return `Result<GenerationOutput, String>`.
  - Only batch size 1 is supported
  - Validates prompt length against `max_seq_len`; returns `Err` if the prompt (after tokenization) exceeds the model's sequence limit
  - `max_new_tokens=0` returns immediately after prefill with no tokens generated
  - `temperature <= 0` forces argmax regardless of the configured sampler
  - Callback-based streaming via `FnMut(GenerationEvent) -> ControlFlow<()>` — callers receive `PrefillProgress`, `Token`, and `Done` events
  - Optional chunked prefill: splits prompt into chunks of `prefill_chunk_size` tokens, reducing peak attention memory from O(N^2) to O(C*N)
  - Prefill chunks use `build_causal_mask(chunk_len, total_seq_len)` with the correct positional offset; KV cache accumulates across chunks. Decode steps (seq_len=1) skip the mask entirely (pass `None`) since a single query can attend to all cached positions.
  - `ControlFlow::Break(())` from the callback cancels generation early (yields `StopReason::Cancelled`)

### MoE Implementation

- `Mlp` enum (`Dense`/`Moe`) makes TransformerBlock polymorphic — each layer is independently dense or MoE
- `MoeLayer` stores packed 3D expert weights (gate_up_proj, down_proj) and a 2D router weight
- During loading, per-expert 2D weights from safetensors are transposed and packed into 3D tensors: gate_proj + up_proj → `gate_up_proj` [num_experts, hidden, 2*moe_intermediate], down_proj → `down_proj` [num_experts, moe_intermediate, hidden]
- Forward: router softmax → top-k selection → optional renormalization (`norm_topk_prob`) → two-path dispatch based on token count
- **Batched decode path** (`forward_batched`): for `num_tokens <= num_experts_per_tok` (decode is always 1 token). Uses GPU-side `select(0, ...)` to gather top-k expert weights, then runs a single batched matmul across all k experts simultaneously. ~6 GPU dispatches per layer instead of ~40. No CPU roundtrip for expert indices.
- **Per-expert prefill path** (`forward_per_expert`): for prefill (many tokens). Builds a dispatch table via `HashMap<expert_idx, Vec<(token_idx, weight)>>` in one pass, then iterates only active experts (typically 8-30 unique for a short prefill) instead of all 128.
- `Qwen3Config::is_moe_layer(i)` determines per-layer type via `decoder_sparse_step` and `mlp_only_layers`

## Model Sizes

### Dense Models

| Model | Layers | Hidden | Heads | KV Heads | Intermediate |
|-------|--------|--------|-------|----------|--------------|
| 0.6B  | 28     | 1024   | 16    | 8        | 3072         |
| 1.7B  | 28     | 1536   | 16    | 8        | 4608         |
| 4B    | 36     | 2560   | 32    | 8        | 9728         |
| 8B    | 36     | 4096   | 32    | 8        | 12288        |

### MoE Models

| Model | Layers | Hidden | Heads | KV Heads | Experts | Top-K | MoE Intermediate |
|-------|--------|--------|-------|----------|---------|-------|------------------|
| 30B-A3B | 48   | 2048   | 32    | 4        | 128     | 8     | 768              |
| 235B-A22B | 94 | 4096   | 64    | 4        | 128     | 8     | 1536             |

## Performance

Qwen3-0.6B on Apple Silicon (M-series, Metal via WGPU): ~15-18 tokens/s, model loads in <1s.

## Qwen3 Chat Template

```
<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\n{message}<|im_end|>\n<|im_start|>assistant\n
```

Special tokens: BOS=151643, EOS=151645. The 0.6B model generates `<think>...</think>` blocks before answering (thinking mode).
